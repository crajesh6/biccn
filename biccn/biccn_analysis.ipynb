{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74b6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Callable, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def one_hot_encode(\n",
    "        sequence: str,\n",
    "        alphabet: str = \"ACGT\",\n",
    "        neutral_alphabet: str = \"N\",\n",
    "        neutral_value: Any = 0,\n",
    "        dtype=np.float32\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"One-hot encode sequence.\"\"\"\n",
    "\n",
    "    def to_uint8(s):\n",
    "        return np.frombuffer(s.encode(\"ascii\"), dtype=np.uint8)\n",
    "\n",
    "    lookup = np.zeros([np.iinfo(np.uint8).max, len(alphabet)], dtype=dtype)\n",
    "    lookup[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
    "    lookup[to_uint8(neutral_alphabet)] = neutral_value\n",
    "    lookup = lookup.astype(dtype)\n",
    "    return lookup[to_uint8(sequence)]\n",
    "\n",
    "\n",
    "class WindowedGenomeDataset:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_path: str,\n",
    "            fasta_path: str,\n",
    "            window_size: int = None,\n",
    "            chromosomes: List[str] = None,\n",
    "            dtype: np.float32 = None\n",
    "        ):\n",
    "        self.target_path = target_path\n",
    "        self.fasta_path = fasta_path\n",
    "        self.chromosomes = chromosomes\n",
    "        self.dtype = dtype\n",
    "\n",
    "        df = pd.read_table(target_path, header=None, sep=\"\\t\")\n",
    "        c = df[0].isin(chromosomes) if chromosomes else np.array(len(df)*[True])\n",
    "        self.windows = df.loc[c, :2].values\n",
    "        self.targets = df.loc[c, 3:].values.astype(self.dtype)\n",
    "\n",
    "        _, start, stop = self.windows[0]\n",
    "        self.window_size = window_size or (stop - start)\n",
    "\n",
    "        self.input_shape = (self.window_size, 4)\n",
    "        self.output_shape = (self.targets.shape[1],)\n",
    "\n",
    "        self.genome = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, item: Union[int, slice]):\n",
    "        if not self.genome:\n",
    "            from pyfaidx import Fasta\n",
    "            self.genome = Fasta(self.fasta_path)\n",
    "\n",
    "        chrom = self.windows[item, 0]\n",
    "        start = self.windows[item, 1]\n",
    "        stop = self.windows[item, 2]\n",
    "\n",
    "        labels = self.targets[item]\n",
    "\n",
    "        if isinstance(item, slice):\n",
    "            seq = \"\"\n",
    "            for chr, i, j in zip(chrom, start, stop):\n",
    "                seq += self.genome[chr][i:j].seq.upper()\n",
    "\n",
    "            inputs = one_hot_encode(seq)\n",
    "            inputs = inputs.reshape(len(chrom), self.window_size, 4)\n",
    "\n",
    "            item = range(item.start or 0, item.stop or len(self), item.step or 1)\n",
    "            item = np.array(list(item), dtype=np.int32)\n",
    "        else:\n",
    "            seq = self.genome[chrom][start:stop].seq.upper()\n",
    "            inputs = one_hot_encode(seq)\n",
    "\n",
    "            item = item if item >= 0 else len(self) + item\n",
    "\n",
    "        output = {\n",
    "            \"inputs\": inputs,\n",
    "            \"labels\": labels,\n",
    "            \"meta\": {\n",
    "                \"id\": item,\n",
    "                \"chrom\": chrom,\n",
    "                \"start\": start,\n",
    "                \"stop\": stop\n",
    "            }\n",
    "        }\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def to_tfrecord(self, path: str, indices: List[int] = None):\n",
    "\n",
    "        def serialize_example(id, inputs, labels, chrom, start, stop):\n",
    "\n",
    "            inputs = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "            inputs = tf.io.serialize_tensor(inputs).numpy()\n",
    "            labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "            labels = tf.io.serialize_tensor(labels).numpy()\n",
    "\n",
    "            chrom = chrom.encode(\"utf-8\")\n",
    "\n",
    "            feature = {\n",
    "                \"id\": tf.train.Feature(int64_list=tf.train.Int64List(value=[id])),\n",
    "                \"inputs\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[inputs])),\n",
    "                \"labels\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[labels])),\n",
    "                \"chrom\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[chrom])),\n",
    "                \"start\": tf.train.Feature(int64_list=tf.train.Int64List(value=[start])),\n",
    "                \"stop\": tf.train.Feature(int64_list=tf.train.Int64List(value=[stop]))\n",
    "            }\n",
    "\n",
    "            example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            return example_proto.SerializeToString()\n",
    "\n",
    "        def generator():\n",
    "            for i in tqdm(indices or range(len(self))):\n",
    "                output = self[i]\n",
    "                yield serialize_example(\n",
    "                    id=output[\"meta\"][\"id\"],\n",
    "                    inputs=output[\"inputs\"],\n",
    "                    labels=output[\"labels\"],\n",
    "                    chrom=output[\"meta\"][\"chrom\"],\n",
    "                    start=output[\"meta\"][\"start\"],\n",
    "                    stop=output[\"meta\"][\"stop\"]\n",
    "                )\n",
    "\n",
    "        serialized_features_dataset = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_types=tf.string,\n",
    "            output_shapes=()\n",
    "        )\n",
    "\n",
    "        writer = tf.data.experimental.TFRecordWriter(path)\n",
    "        writer.write(serialized_features_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc38d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/chandana/projects/biccn/data/\"\n",
    "SAVE_DIR = \"/home/chandana/projects/biccn/models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8006e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    (\"train\", [2, 4, 6, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, \"X\"]),\n",
    "    (\"valid\", [7, 9]),\n",
    "    (\"test\", [1, 3, 5])\n",
    "]\n",
    "\n",
    "save = False\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for name, chroms in items:\n",
    "    datasets[name] = WindowedGenomeDataset(\n",
    "        target_path=f\"{DATA_DIR}/targets.bed\",\n",
    "        fasta_path=f\"{DATA_DIR}/GRCm38.fa\",\n",
    "        chromosomes=[f\"chr{i}\" for i in chroms],\n",
    "        window_size=999,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    if save:\n",
    "        datasets[name].to_tfrecord(f\"/home/chandana/projects/biccn/data/{name}.tfrec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ce10c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c957bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4af9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "THRESHOLD = 0.0006867924257022952\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "        \n",
    "\n",
    "def parse_tfrecord(example):\n",
    "    feature_description = {\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"inputs\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"labels\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"chrom\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"start\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"stop\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    \n",
    "    record = tf.io.parse_single_example(example, feature_description)\n",
    "    record[\"inputs\"] = tf.io.parse_tensor(record[\"inputs\"], out_type=tf.float32)\n",
    "    record[\"labels\"] = tf.io.parse_tensor(record[\"labels\"], out_type=tf.float32)\n",
    "\n",
    "    inputs = tf.reshape(record[\"inputs\"] , [999, 4])\n",
    "    labels = tf.reshape(record[\"labels\"], [33,])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def load_dataset(filenames):\n",
    "\n",
    "    records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "    records = records.map(parse_tfrecord, num_parallel_calls=AUTO)\n",
    "\n",
    "    def data_augment(inputs, labels):\n",
    "        return inputs, tf.where(labels > THRESHOLD, 1.0, 0.0)\n",
    "\n",
    "\n",
    "    return records.map(data_augment, num_parallel_calls=AUTO)\n",
    "\n",
    "\n",
    "def get_dataset(batch_size: int, name: str):\n",
    "\n",
    "    if name == \"train\":\n",
    "        dataset = load_dataset([DATA_DIR + \"/train.tfrec\"])\n",
    "        dataset = dataset.shuffle(10000)\n",
    "        dataset = dataset.repeat()\n",
    "    elif name == \"valid\":\n",
    "        dataset = load_dataset([DATA_DIR + \"/valid.tfrec\"])\n",
    "        dataset = dataset.repeat()\n",
    "    elif name == \"test\":\n",
    "        dataset = load_dataset([DATA_DIR + \"/test.tfrec\"])\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(AUTO) \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eed18ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.7026630169507466\n",
      "Validation:  0.10160312906924136\n",
      "Validation:  0.19573385398001197\n"
     ]
    }
   ],
   "source": [
    "train_size = len(datasets['train'])\n",
    "valid_size = len(datasets['valid'])\n",
    "test_size = len(datasets['test'])\n",
    "\n",
    "total = train_size + valid_size + test_size\n",
    "\n",
    "print(\"Train: \", train_size / total)\n",
    "print(\"Validation: \", valid_size / total)\n",
    "print(\"Validation: \", test_size / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7788675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 16:03:49.023627: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 16:03:49.796372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9653 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_dataset(BATCH_SIZE, name=\"train\")\n",
    "valid_ds = get_dataset(BATCH_SIZE, name=\"valid\")\n",
    "test_ds = get_dataset(BATCH_SIZE, name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db06a27",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bf64a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevCompConv1D(tf.keras.layers.Conv1D):\n",
    "  \"\"\"\n",
    "  Implement forward and reverse-complement filter convolutions\n",
    "  for 1D signals. It takes as input either a single input or two inputs\n",
    "  (where the second input is the reverse complement scan). If a single input,\n",
    "  this performs both forward and reverse complement scans and either merges it\n",
    "  (if concat=True) or returns a separate scan for forward and reverse comp.\n",
    "  \"\"\"\n",
    "  def __init__(self, *args, concat=False, **kwargs):\n",
    "    super(RevCompConv1D, self).__init__(*args, **kwargs)\n",
    "    self.concat = concat\n",
    "\n",
    "\n",
    "  def call(self, inputs, inputs2=None):\n",
    "\n",
    "    if inputs2 is not None:\n",
    "      # create rc_kernels\n",
    "      rc_kernel = self.kernel[::-1,::-1,:]\n",
    "\n",
    "      # convolution 1D\n",
    "      outputs = self._convolution_op(inputs, self.kernel)\n",
    "      rc_outputs = self._convolution_op(inputs2, rc_kernel)\n",
    "\n",
    "    else:\n",
    "      # create rc_kernels\n",
    "      rc_kernel = tf.concat([self.kernel, self.kernel[::-1,::-1,:]], axis=-1)\n",
    "\n",
    "      # convolution 1D\n",
    "      outputs = self._convolution_op(inputs, rc_kernel)\n",
    "\n",
    "      # unstack to forward and reverse strands\n",
    "      outputs = tf.unstack(outputs, axis=2)\n",
    "      rc_outputs = tf.stack(outputs[self.filters:], axis=2)\n",
    "      outputs = tf.stack(outputs[:self.filters], axis=2)\n",
    "\n",
    "    # add bias\n",
    "    if self.use_bias:\n",
    "      outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "      rc_outputs = tf.nn.bias_add(rc_outputs, self.bias)\n",
    "\n",
    "    # add activations\n",
    "    if self.activation is not None:\n",
    "      outputs = self.activation(outputs)\n",
    "      rc_outputs = self.activation(rc_outputs)\n",
    "\n",
    "    if self.concat:\n",
    "      return tf.concat([outputs, rc_outputs], axis=-1)\n",
    "    else:\n",
    "      return outputs, rc_outputs\n",
    "\n",
    "\n",
    "def dilated_residual_block(input_layer, filter_size, rates):\n",
    "    num_filters = input_layer.shape.as_list()[-1]\n",
    "    nn = tf.keras.layers.Conv1D(\n",
    "        filters=num_filters,\n",
    "        kernel_size=filter_size,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        padding='same',\n",
    "        dilation_rate=rates[0],\n",
    "    )(input_layer)\n",
    "    nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "\n",
    "    for f in rates[1:]:\n",
    "        nn = tf.keras.layers.Activation('relu')(nn)\n",
    "        nn = tf.keras.layers.Dropout(0.1)(nn)\n",
    "        nn = tf.keras.layers.Conv1D(\n",
    "            filters=num_filters,\n",
    "            kernel_size=filter_size,\n",
    "            strides=1,\n",
    "            activation=None,\n",
    "            use_bias=False,\n",
    "            padding='same',\n",
    "            dilation_rate=f\n",
    "        )(nn)\n",
    "\n",
    "    nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "    nn = tf.keras.layers.add([input_layer, nn])\n",
    "    return tf.keras.layers.Activation('relu')(nn)\n",
    "\n",
    "\n",
    "def residualbind(input_shape, output_shape, activation='exponential', num_units=[128, 256, 512, 512], rc=False):\n",
    "\n",
    "    # input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # layer 1\n",
    "    if rc:\n",
    "        nn = RevCompConv1D(filters=num_units[0], kernel_size=19, use_bias=False, padding='same', concat=True)(inputs)\n",
    "    else:\n",
    "        nn = tf.keras.layers.Conv1D(\n",
    "            filters=num_units[0],\n",
    "            kernel_size=19,\n",
    "            strides=1,\n",
    "            activation=None,\n",
    "            use_bias=False,\n",
    "            padding='same'\n",
    "        )(inputs)\n",
    "\n",
    "    nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "    nn = tf.keras.layers.Activation(activation)(nn)\n",
    "    nn = tf.keras.layers.Dropout(0.1)(nn)\n",
    "\n",
    "    # dilated residual block\n",
    "    nn = dilated_residual_block(nn, filter_size=3, rates=[1, 2, 4, 8])\n",
    "    nn = tf.keras.layers.MaxPooling1D(pool_size=10)(nn) # 500\n",
    "    nn = tf.keras.layers.Dropout(0.2)(nn)\n",
    "\n",
    "    # layer 2\n",
    "    nn = tf.keras.layers.Conv1D(\n",
    "        filters=num_units[1],\n",
    "        kernel_size=7,\n",
    "        strides=1,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        padding='same',\n",
    "    )(nn)\n",
    "    nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "    nn = tf.keras.layers.Activation('relu')(nn)\n",
    "    nn = tf.keras.layers.Dropout(0.1)(nn)\n",
    "\n",
    "    # dilated residual block\n",
    "    nn = dilated_residual_block(nn, filter_size=3, rates=[1, 2, 4])\n",
    "    nn = tf.keras.layers.MaxPooling1D(pool_size=10)(nn) # 50\n",
    "    nn = tf.keras.layers.Dropout(0.2)(nn)\n",
    "\n",
    "  # layer 2\n",
    "    nn = tf.keras.layers.Conv1D(\n",
    "        filters=num_units[2],\n",
    "        kernel_size=7,\n",
    "        strides=1,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        padding='same',\n",
    "    )(nn)\n",
    "    nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "    nn = tf.keras.layers.Activation('relu')(nn)\n",
    "\n",
    "    nn = tf.keras.layers.GlobalAveragePooling1D()(nn) # 1\n",
    "    nn = tf.keras.layers.Dropout(0.3)(nn)\n",
    "\n",
    "    # Fully-connected NN\n",
    "    nn = tf.keras.layers.Flatten()(nn)\n",
    "    nn = tf.keras.layers.Dense(num_units[3], activation=None, use_bias=False)(nn)\n",
    "    nn = tf.keras.layers.BatchNormalization()(nn)\n",
    "    nn = tf.keras.layers.Activation('relu')(nn)\n",
    "    nn = tf.keras.layers.Dropout(0.5)(nn)\n",
    "\n",
    "    # output layer\n",
    "    logits = tf.keras.layers.Dense(output_shape, activation='linear', use_bias=True)(nn)\n",
    "    outputs = tf.keras.layers.Activation('sigmoid')(logits)\n",
    "\n",
    "    # create and return model\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e67aa1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab638d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = residualbind(\n",
    "    input_shape=(999, 4),\n",
    "    output_shape=33,\n",
    "    activation=\"exponential\", \n",
    "    num_units=[128, 256, 512, 512],\n",
    "    rc=False\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.0),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(curve=\"ROC\", name=\"auroc\"),\n",
    "        tf.keras.metrics.AUC(curve=\"PR\", name=\"aupr\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab098a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "100/100 [==============================] - 14s 99ms/step - loss: 0.6911 - auroc: 0.6761 - aupr: 0.5988 - val_loss: 0.4691 - val_auroc: 0.5641 - val_aupr: 0.1743 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.6029 - auroc: 0.7436 - aupr: 0.6607 - val_loss: 0.5598 - val_auroc: 0.5876 - val_aupr: 0.1969 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "train_steps = train_size // BATCH_SIZE\n",
    "valid_steps = valid_size // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=2,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_data=valid_ds,\n",
    "        validation_steps=valid_steps,\n",
    "        callbacks=[\n",
    "             tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_aupr\",\n",
    "                patience=10,\n",
    "                verbose=1,\n",
    "                mode=\"max\",\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_aupr\",\n",
    "                factor=0.2,\n",
    "                patience=3,\n",
    "                min_lr=1e-7,\n",
    "                mode=\"max\",\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03565274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"/home/chandana/projects/biccn/models/resbind_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22e80ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 16:10:22.508248: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3771/3771 [==============================] - 80s 20ms/step - loss: 0.5971 - auroc: 0.7819 - aupr: 0.7208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5970627665519714, 0.7819393873214722, 0.7208306789398193]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds, steps=test_size // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02a9a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
